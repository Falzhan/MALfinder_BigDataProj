{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://myanimelist.net/anime/52991/Sousou_no_Frieren\n"
     ]
    }
   ],
   "source": [
    "# Try fetching from first anime from the first page of top anime\n",
    "nth_anime = 70 - 1# Indexing start at 0\n",
    "page = nth_anime - nth_anime%50\n",
    "nth_anime = nth_anime%50\n",
    "response = requests.get(f'https://myanimelist.net/topanime.php?limit={page}')\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "tags = soup.find_all('tr', {\"class\": \"ranking-list\"})\n",
    "url = BeautifulSoup(str(tags[nth_anime]), 'html.parser').find('a')['href']\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Synonyms': ' Frieren at the Funeral  ',\n",
       " 'Japanese': ' 葬送のフリーレン  ',\n",
       " 'Type': 'TV',\n",
       " 'Episodes': '  28  ',\n",
       " 'Status': '  Finished Airing  ',\n",
       " 'Aired': '  Sep 29, 2023 to Mar 22, 2024  ',\n",
       " 'Premiered': 'Fall 2023',\n",
       " 'Producers': 'Aniplex,       Dentsu,       Shogakukan-Shueisha Productions,       Nippon Television Network,       TOHO animation,       Shogakukan ',\n",
       " 'Licensors': '    None found, add some ',\n",
       " 'Studios': 'Madhouse ',\n",
       " 'Source': '      Manga    ',\n",
       " 'Genres': 'AdventureAdventure,         DramaDrama,         FantasyFantasy ',\n",
       " 'Demographic': 'ShounenShounen ',\n",
       " 'Duration': '  24 min. per ep.  ',\n",
       " 'Rating': '  PG-13 - Teens 13 or older  ',\n",
       " 'Score': '9.341 (scored by 469934469,934 users)      1          indicates a weighted score.    ',\n",
       " 'Ranked': \"  #122    based on the top anime page. Please note that 'Not yet aired' and 'R18+' titles are excluded.    \",\n",
       " 'Popularity': '  #224',\n",
       " 'Members': '    830,523',\n",
       " 'Favorites': '  48,741',\n",
       " 'description': 'During their decade-long quest to defeat the Demon King, the members of the hero\\'s party—Himmel himself, the priest Heiter, the dwarf warrior Eisen, and the elven mage Frieren—forge bonds through adventures and battles, creating unforgettable precious memories for most of them.\\n\\r\\nHowever, the time that Frieren spends with her comrades is equivalent to merely a fraction of her life, which has lasted over a thousand years. When the party disbands after their victory, Frieren casually returns to her \"usual\" routine of collecting spells across the continent. Due to her different sense of time, she seemingly holds no strong feelings toward the experiences she went through.\\n\\r\\nAs the years pass, Frieren gradually realizes how her days in the hero\\'s party truly impacted her. Witnessing the deaths of two of her former companions, Frieren begins to regret having taken their presence for granted; she vows to better understand humans and create real personal connections. Although the story of that once memorable journey has long ended, a new tale is about to begin.\\n\\r\\n[Written by MAL Rewrite]',\n",
       " 'Link': 'https://myanimelist.net/anime/52991/Sousou_no_Frieren'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get information about that anime\n",
    "sub_rq = requests.get(url)\n",
    "soup = BeautifulSoup(sub_rq.text, 'html.parser')\n",
    "\n",
    "out_dict = {}\n",
    "\n",
    "table_tags_list = soup.find_all(attrs={\"class\":'spaceit_pad'})\n",
    "\n",
    "\n",
    "for i in table_tags_list:\n",
    "    s:str = i.text\n",
    "    s = s.replace('\\n','')\n",
    "    \n",
    "    s = s.split(':')\n",
    "    if len(s) == 2:\n",
    "        out_dict[s[0]] = s[1]\n",
    "\n",
    "out_dict[\"description\"] = soup.find(attrs={\"itemprop\":'description'}).text\n",
    "out_dict['Link'] = url\n",
    "out_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "import sys\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random user-agent\n",
    "def get_random_user_agent():\n",
    "    user_agents = [\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    ]\n",
    "    return random.choice(user_agents)\n",
    "\n",
    "# Make a request with retries\n",
    "def make_request(url):\n",
    "    headers = {'User-Agent': get_random_user_agent()}\n",
    "    session = requests.Session()\n",
    "\n",
    "    retry = Retry(\n",
    "        total=5, # Total number of retries\n",
    "        backoff_factor=1, # Exponential backoff factor\n",
    "        status_forcelist=[500, 502, 503, 504], # Retry on these HTTP status codes\n",
    "        method_whitelist=[\"HEAD\", \"GET\", \"OPTIONS\"] # Retry on these HTTP methods\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "\n",
    "    try:\n",
    "        response = session.get(url, headers=headers)\n",
    "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx and 5xx)\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return None\n",
    "    \n",
    "# Create logger\n",
    "def get_logger(logname, log_level=logging.INFO):\n",
    "    # If log file doesn't exist, create new one\n",
    "    if not os.path.exists(logname):\n",
    "        logger = logging.getLogger()\n",
    "        fhandler = logging.FileHandler(filename=logname, encoding=\"utf-8\")\n",
    "        formatter = logging.Formatter('%(asctime)s [%(levelname)s] > %(message)s')\n",
    "        fhandler.setFormatter(formatter)\n",
    "        logger.addHandler(fhandler)\n",
    "        logger.setLevel(log_level)\n",
    "        print(\"Created log \" + logname)\n",
    "\n",
    "    # Else use the existing file\n",
    "    else:\n",
    "        # Configure the logger\n",
    "        logging.basicConfig(\n",
    "            filename= logname,    # Use the existing log file\n",
    "            filemode='a',          # Append mode, to add to the existing log file. 'w' for overwrite\n",
    "            format='%(asctime)s [%(levelname)s] - %(name)s:%(filename)s > %(message)s', # Format using %() for backward compatibility\n",
    "            level=log_level    # Set the logging level as needed. DEBUG means log everything above DEBUG\n",
    "        )\n",
    "\n",
    "        # Create a logger object\n",
    "        logger = logging.getLogger()\n",
    "        print(\"Loaded log \" + logname)\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pickle to store the information so we can continue the collection later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exist\n",
      "16765\n"
     ]
    }
   ],
   "source": [
    "# Pickle the list so we can continue from where the loop last fail\n",
    "pname = \"Data/out_dict_list.pkl\" # Pickle Filenamae\n",
    "\n",
    "# Check if pickle file exist\n",
    "if not os.path.exists(pname):\n",
    "    # Create file\n",
    "    with open(pname, 'wb') as f:\n",
    "        pickle.dump([], f)\n",
    "\n",
    "else:\n",
    "    print(\"File already exist\")\n",
    "\n",
    "# Load from a file\n",
    "with open(pname, 'rb') as f:\n",
    "    out_dict_list = pickle.load(f)\n",
    "\n",
    "last_page = (len(out_dict_list)//50)*50\n",
    "print(len(out_dict_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get logger so we can log the data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded log AnimeScrape.log\n"
     ]
    }
   ],
   "source": [
    "logger = get_logger(\"AnimeScrape.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape MyAnimeList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f86b212880834192ae213c7ab604f5f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nadzm\\AppData\\Local\\Temp\\ipykernel_25152\\755529457.py:15: DeprecationWarning: Using 'method_whitelist' with Retry is deprecated and will be removed in v2.0. Use 'allowed_methods' instead\n",
      "  retry = Retry(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25152\\3683530201.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[0mpbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m                 \u001b[1;31m# Introduce a random delay between requests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                 \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Delay between 1 to 3 seconds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"Failed to process target {i} on page {page}: {e}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "verbose = False # If true print the progress\n",
    "end_at = 17000\n",
    "\n",
    "# Progress Bar\n",
    "pbar = tqdm(total=end_at - last_page)\n",
    "\n",
    "# Main scraping loop\n",
    "for page in range(last_page, end_at, 50):\n",
    "    try:\n",
    "        # Fetch the page with the list of top anime\n",
    "        response = make_request(f'https://myanimelist.net/topanime.php?limit={page}')\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response is None:\n",
    "            break\n",
    "\n",
    "        # Parse the page content\n",
    "        soup = BeautifulSoup(response, 'html.parser')\n",
    "        tags = soup.find_all('tr', {\"class\": \"ranking-list\"})\n",
    "        \n",
    "        for i, tag in enumerate(tags):\n",
    "            try:\n",
    "                # Extract the URL of the anime detail page\n",
    "                url = tag.find('a')['href']\n",
    "                \n",
    "                # Fetch the anime detail page\n",
    "                sub_response = make_request(url)\n",
    "                if sub_response is None:\n",
    "                    continue\n",
    "                \n",
    "                # Parse the detail page content\n",
    "                sub_soup = BeautifulSoup(sub_response, 'html.parser')\n",
    "                \n",
    "                # Find all relevant tags\n",
    "                table_tags_list = sub_soup.find_all(attrs={\"class\": 'spaceit_pad'})\n",
    "                \n",
    "                out_dict = {}\n",
    "                for j in table_tags_list:\n",
    "                    s: str = j.text.replace('\\n', '')\n",
    "                    s_split: list[str] = s.split(':')\n",
    "\n",
    "                    # Check if the split was successful and store the key-value pair\n",
    "                    if len(s_split) == 2:\n",
    "                        out_dict[s_split[0].strip()] = s_split[1].strip()\n",
    "                        \n",
    "                \n",
    "                out_dict['url'] = url\n",
    "                #######\n",
    "                #Extra Features:\n",
    "                    # Extract the description\n",
    "                description_tag = sub_soup.find(attrs={\"itemprop\": 'description'})\n",
    "                out_dict[\"description\"] = description_tag.text if description_tag else \"\"\n",
    "                    # Name\n",
    "                out_dict['Name'] = sub_soup.find(attrs={\"class\": 'title-name h1_bold_none'}).text\n",
    "                if not (sub_soup.find(attrs={\"class\": 'title-english title-inherit'}) is None):\n",
    "                    out_dict['English Name'] = sub_soup.find(attrs={\"class\": 'title-english title-inherit'}).text\n",
    "                    # Reviews\n",
    "                        # Recommended\n",
    "                rev_R = sub_soup.find(attrs={\"class\": 'recommended'})\n",
    "                out_dict['Recommended'] =rev_R.text.replace('\\n','')\n",
    "                        # MixedFeelings\n",
    "                rev_M = sub_soup.find(attrs={\"class\": 'mixed-feelings'})\n",
    "                out_dict['Mixed Feelings'] =rev_M.text.replace('\\n','')\n",
    "                        # NotRecommended\n",
    "                rev_M = sub_soup.find(attrs={\"class\": 'not-recommended'})\n",
    "                out_dict['Not Recommended'] =rev_M.text.replace('\\n','')\n",
    "                \n",
    "                # Log the progress and print progress information\n",
    "                msg = f\"Page {page}, Target {i}, Name: {out_dict.get('Name', 'N/A')}\"\n",
    "                if verbose:\n",
    "                    print(msg)\n",
    "                logger.info(msg)\n",
    "                \n",
    "                # Add the dictionary to the list\n",
    "                out_dict_list.append(out_dict)\n",
    "                \n",
    "                pbar.update(1)\n",
    "                # Introduce a random delay between requests\n",
    "                time.sleep(random.uniform(0.5, 2)) # Delay between 0.5 to 2 seconds\n",
    "            except Exception as e:\n",
    "                msg = f\"Failed to process target {i} on page {page}: {e}\"\n",
    "                print(msg)\n",
    "                logger.error(msg)\n",
    "        \n",
    "    except Exception as e:\n",
    "        with open(pname, 'wb') as f:\n",
    "            pickle.dump(out_dict_list, f)\n",
    "        msg = f\"Failed to process page {page}: {e}\"\n",
    "        print(msg)\n",
    "        logger.error(msg)\n",
    "\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn into DataFrame and Save as .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(out_dict_list)\n",
    "df.to_csv(\"Data/AnimeRaw.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

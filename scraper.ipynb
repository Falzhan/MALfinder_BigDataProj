{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "120%50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nth_anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://myanimelist.net/anime/52991/Sousou_no_Frieren\n"
     ]
    }
   ],
   "source": [
    "# Try fetching from first anime from the first page of top anime\n",
    "nth_anime = 70 - 1# Indexing start at 0\n",
    "page = nth_anime - nth_anime%50\n",
    "nth_anime = nth_anime%50\n",
    "response = requests.get(f'https://myanimelist.net/topanime.php?limit={page}')\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "tags = soup.find_all('tr', {\"class\": \"ranking-list\"})\n",
    "url = BeautifulSoup(str(tags[nth_anime]), 'html.parser').find('a')['href']\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Synonyms': ' Frieren at the Funeral  ',\n",
       " 'Japanese': ' 葬送のフリーレン  ',\n",
       " 'Type': 'TV',\n",
       " 'Episodes': '  28  ',\n",
       " 'Status': '  Finished Airing  ',\n",
       " 'Aired': '  Sep 29, 2023 to Mar 22, 2024  ',\n",
       " 'Premiered': 'Fall 2023',\n",
       " 'Producers': 'Aniplex,       Dentsu,       Shogakukan-Shueisha Productions,       Nippon Television Network,       TOHO animation,       Shogakukan ',\n",
       " 'Licensors': '    None found, add some ',\n",
       " 'Studios': 'Madhouse ',\n",
       " 'Source': '      Manga    ',\n",
       " 'Genres': 'AdventureAdventure,         DramaDrama,         FantasyFantasy ',\n",
       " 'Demographic': 'ShounenShounen ',\n",
       " 'Duration': '  24 min. per ep.  ',\n",
       " 'Rating': '  PG-13 - Teens 13 or older  ',\n",
       " 'Score': '9.341 (scored by 469934469,934 users)      1          indicates a weighted score.    ',\n",
       " 'Ranked': \"  #122    based on the top anime page. Please note that 'Not yet aired' and 'R18+' titles are excluded.    \",\n",
       " 'Popularity': '  #224',\n",
       " 'Members': '    830,523',\n",
       " 'Favorites': '  48,741',\n",
       " 'description': 'During their decade-long quest to defeat the Demon King, the members of the hero\\'s party—Himmel himself, the priest Heiter, the dwarf warrior Eisen, and the elven mage Frieren—forge bonds through adventures and battles, creating unforgettable precious memories for most of them.\\n\\r\\nHowever, the time that Frieren spends with her comrades is equivalent to merely a fraction of her life, which has lasted over a thousand years. When the party disbands after their victory, Frieren casually returns to her \"usual\" routine of collecting spells across the continent. Due to her different sense of time, she seemingly holds no strong feelings toward the experiences she went through.\\n\\r\\nAs the years pass, Frieren gradually realizes how her days in the hero\\'s party truly impacted her. Witnessing the deaths of two of her former companions, Frieren begins to regret having taken their presence for granted; she vows to better understand humans and create real personal connections. Although the story of that once memorable journey has long ended, a new tale is about to begin.\\n\\r\\n[Written by MAL Rewrite]',\n",
       " 'Link': 'https://myanimelist.net/anime/52991/Sousou_no_Frieren'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get information about that anime\n",
    "sub_rq = requests.get(url)\n",
    "soup = BeautifulSoup(sub_rq.text, 'html.parser')\n",
    "\n",
    "out_dict = {}\n",
    "\n",
    "table_tags_list = soup.find_all(attrs={\"class\":'spaceit_pad'})\n",
    "\n",
    "\n",
    "for i in table_tags_list:\n",
    "    s:str = i.text\n",
    "    s = s.replace('\\n','')\n",
    "    \n",
    "    s = s.split(':')\n",
    "    if len(s) == 2:\n",
    "        out_dict[s[0]] = s[1]\n",
    "\n",
    "out_dict[\"description\"] = soup.find(attrs={\"itemprop\":'description'}).text\n",
    "out_dict['Link'] = url\n",
    "out_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a random user-agent\n",
    "def get_random_user_agent():\n",
    "    user_agents = [\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    ]\n",
    "    return random.choice(user_agents)\n",
    "\n",
    "#make a request with retries\n",
    "def make_request(url):\n",
    "    headers = {'User-Agent': get_random_user_agent()}\n",
    "    session = requests.Session()\n",
    "\n",
    "    retry = Retry(\n",
    "        total=5, # Total number of retries\n",
    "        backoff_factor=1, # Exponential backoff factor\n",
    "        status_forcelist=[500, 502, 503, 504], # Retry on these HTTP status codes\n",
    "        method_whitelist=[\"HEAD\", \"GET\", \"OPTIONS\"] # Retry on these HTTP methods\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "\n",
    "    try:\n",
    "        response = session.get(url, headers=headers)\n",
    "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx and 5xx)\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the list so we can continue from where the loop last fail\n",
    "pname = \"out_dict.pkl\" # Pickle Filenamae\n",
    "\n",
    "# Check if pickle file exist\n",
    "if not os.path.exists(pname):\n",
    "    # Create file\n",
    "    with open(pname, 'wb') as f:\n",
    "        pickle.dump([], f)\n",
    "\n",
    "else:\n",
    "    print(\"File already exist\")\n",
    "\n",
    "# Load from a file\n",
    "with open(pname, 'rb') as f:\n",
    "    out_dict_list = pickle.load(f)\n",
    "\n",
    "page = len(out_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main scraping loop\n",
    "while True:\n",
    "    try:\n",
    "        # Fetch the page with the list of top anime\n",
    "        response = make_request(f'https://myanimelist.net/topanime.php?limit={page}')\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response is None:\n",
    "            break\n",
    "\n",
    "        # Parse the page content\n",
    "        soup = BeautifulSoup(response, 'html.parser')\n",
    "        tags = soup.find_all('tr', {\"class\": \"ranking-list\"})\n",
    "        \n",
    "        for i, tag in enumerate(tags):\n",
    "            try:\n",
    "                # Extract the URL of the anime detail page\n",
    "                print(tag.find('a')[\"href\"])\n",
    "                url = tag.find('a')['href']\n",
    "                \n",
    "                # Fetch the anime detail page\n",
    "                sub_response = make_request(url)\n",
    "                if sub_response is None:\n",
    "                    continue\n",
    "                \n",
    "                # Parse the detail page content\n",
    "                sub_soup = BeautifulSoup(sub_response, 'html.parser')\n",
    "                \n",
    "                # Find all relevant tags\n",
    "                table_tags_list = sub_soup.find_all(attrs={\"class\": 'spaceit_pad'})\n",
    "                \n",
    "                out_dict = {}\n",
    "                for j in table_tags_list:\n",
    "                    s: str = j.text.replace('\\n', '')\n",
    "                    s_split: list[str] = s.split(':')\n",
    "\n",
    "                    # Check if the split was successful and store the key-value pair\n",
    "                    if len(s_split) == 2:\n",
    "                        out_dict[s_split[0].strip()] = s_split[1].strip()\n",
    "                        \n",
    "                \n",
    "                out_dict['url'] = url\n",
    "                #######\n",
    "                #Extra Features:\n",
    "                    # Extract the description\n",
    "                description_tag = sub_soup.find(attrs={\"itemprop\": 'description'})\n",
    "                out_dict[\"description\"] = description_tag.text if description_tag else \"\"\n",
    "                    # Name\n",
    "                out_dict['Name'] = sub_soup.find(attrs={\"class\": 'title-name h1_bold_none'}).text\n",
    "                if not (sub_soup.find(attrs={\"class\": 'title-english title-inherit'}) is None):\n",
    "                    out_dict['English Name'] = sub_soup.find(attrs={\"class\": 'title-english title-inherit'}).text\n",
    "                    # Reviews\n",
    "                        # Recommended\n",
    "                rev_R = sub_soup.find(attrs={\"class\": 'recommended'})\n",
    "                out_dict['Recommended'] =rev_R.text.replace('\\n','')\n",
    "                        # MixedFeelings\n",
    "                rev_M = sub_soup.find(attrs={\"class\": 'mixed-feelings'})\n",
    "                out_dict['Mixed Feelings'] =rev_M.text.replace('\\n','')\n",
    "                        # NotRecommended\n",
    "                rev_M = sub_soup.find(attrs={\"class\": 'not-recommended'})\n",
    "                out_dict['Not Recommended'] =rev_M.text.replace('\\n','')\n",
    "                \n",
    "                # Print progress information\n",
    "                print(f\"page {page}, target {i}, name {out_dict.get('Name', 'N/A')}\")\n",
    "                \n",
    "                # Add the dictionary to the list\n",
    "                out_dict_list.append(out_dict)\n",
    "                \n",
    "                # Introduce a random delay between requests\n",
    "                time.sleep(random.uniform(2, 5)) # Delay between 2 to 5 seconds\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process target {i} on page {page}: {e}\")\n",
    "        \n",
    "        # Increment the page for the next iteration\n",
    "        page += 50\n",
    "    except Exception as e:\n",
    "        with open(pname, 'wb') as f:\n",
    "            pickle.dump(out_dict_list, f)\n",
    "        print(f\"Failed to process page {page}: {e}\")\n",
    "        break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
